
# Install the required libraries
#!pip install opencv-python ultralytics ipython
import cv2
from ultralytics import YOLO
# model = YOLO('yolov8n.pt')
model = YOLO('yolov8m.pt')
# --- Initialize Webcam ---
try:
 # Open a connection to the default camera (camera index 0)
 cap = cv2.VideoCapture(0)
 if not cap.isOpened():
 raise IOError("Cannot open webcam. Please check if it is connected 
except Exception as e:
 print(f"Error: {e}")
 exit()
# Set a preferred camera resolution
cap.set(3, 1280) # Width
cap.set(4, 7
 20) # Height
# A set to store all unique object classes detected during the session
all_detected_objects = set()
print("Starting live detection... Press 'q' in the display window to quit."
# --- Main Loop for Detection ---
while True:
 # Read a frame from the camera
 success, frame = cap.read()
 if not success:
 print("Failed to grab a frame. Exiting...")
 break
 # Perform object detection on the current frame
 results = model(frame)
 
# Iterate over the detected objects in the current frame
 for result in results:
 boxes = result.boxes
 for box in boxes:
 # Get class ID and name
 class_id = int(box.cls[0])
 class_name = model.names[class_id]
 # Add the detected class name to our master set
 all_detected_objects.add(class_name)
 # --- Visualization ---
 # Get confidence score and coordinates
 confidence = float(box.conf[0])
 x1, y1, x2, y2 = map(int, box.xyxy[0])
 
# Draw the bounding box on the frame
 cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
 # Create the label text
 label = f'{class_name} {confidence:.2f}'
 # Put the label on the bounding box
 cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPL
 
# Display the resulting frame in a window named 'Live Object Detection'
 cv2.imshow('Live Object Detection', frame)
 # Check if the 'q' key is pressed. If yes, break the loop.
 # cv2.waitKey(1) waits 1ms for a key event. & 0xFF is a standard mask.
 if cv2.waitKey(1) & 0xFF == ord('q'):
 break
 
# --- Cleanup and Final Report ---
print("\nDetection stopped by user.")
# Release the camera and destroy all OpenCV windows
cap.release()
cv2.destroyAllWindows()
# Print the final list of all unique objects detected during the session
if all_detected_objects:
 print("\n--- Summary of All Unique Objects Detected ---")
 # Sort the list alphabetically for clean output
 for i, obj in enumerate(sorted(list(all_detected_objects)), 1):
 print(f"{i}. {obj}")
 print("--------------------------------------------")
else:
 print("\nNo objects were detected during the session.")








yashikaüëáüèª








# yolo



!pip install ultralytics

#fornimage 

from ultralytics import YOLO

model = YOLO("yolov8n.pt")


results = model("/content/image_yolo.jpg")

# Show detection results (opens image window)
results.show()

# Optional: Save the output image
results.save(filename="output.jpg")

print("‚úÖ Object detection complete! Saved as output.jpg")


#for webcam
# save as yolov8_webcam.py
import cv2
from ultralytics import YOLO
import time

# load model (downloads automatically if missing)
model = YOLO("yolov8n.pt")  # change to yolov8s.pt for more accuracy

cap = cv2.VideoCapture(0)  # change index if needed
if not cap.isOpened():
    raise RuntimeError("Cannot open webcam")

fps_time = time.time()
while True:
    ret, frame = cap.read()
    if not ret:
        break

    # run inference (returns a Results object or list)
    results = model(frame, conf=0.25, verbose=False)  # single-frame inference
    r = results[0]  # first result

    # r.boxes contains detections; each box has xyxy + conf + cls
    if hasattr(r, "boxes") and r.boxes is not None:
        for box in r.boxes:
            xyxy = box.xyxy[0].cpu().numpy()          # [x1,y1,x2,y2]
            conf = float(box.conf[0].cpu().numpy())
            cls_id = int(box.cls[0].cpu().numpy())
            label = model.names[cls_id] if hasattr(model, "names") else str(cls_id)
            x1, y1, x2, y2 = map(int, xyxy)
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)
            cv2.putText(frame, f"{label} {conf:.2f}", (x1, y1-6),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 1)

    # display FPS
    fps = 1.0 / (time.time() - fps_time)
    fps_time = time.time()
    cv2.putText(frame, f"FPS: {fps:.1f}", (10,30),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,0,0), 2)

    cv2.imshow("YOLOv8 Webcam", frame)
    key = cv2.waitKey(1) & 0xFF
    if key == ord("q"):
        break
    if key == ord("s"):
        # save snapshot
        cv2.imwrite(f"snapshot_{int(time.time())}.jpg", frame)
        print("Snapshot saved")

cap.release()
cv2.destroyAllWindows()



Q1. What is YOLO?

A: YOLO stands for You Only Look Once ‚Äî it‚Äôs a deep learning algorithm that detects multiple objects in an image in a single forward pass, making it fast and accurate.

üîπ Q2. What does this Python code do?

A: It loads a pre-trained YOLOv8 model, detects objects in an image (image_yolo.jpg), then displays and saves the image with bounding boxes and labels drawn on it.

üîπ Q3. What library is used here?

A: The code uses the Ultralytics YOLO library ‚Äî it makes using YOLOv8 very easy in Python. We install it using:

pip install ultralytics

üîπ Q4. What does this line do? model = YOLO("yolov8n.pt")

A: It loads a small, pre-trained YOLOv8 model (n = nano version). This model already knows how to detect 80+ common objects (like person, car, dog, etc).

üîπ Q5. What does this line do? results = model("image_yolo.jpg")

A: It sends the image to the model for object detection. The result contains all detected objects, their positions, and confidence scores.

üîπ Q6. Why is there a for r in results: loop?

A: Because results is a list (it can have multiple images or frames). We loop through each result to display and save it.

üîπ Q7. What do r.show() and r.save() do?

A:

r.show() ‚Üí Opens a window showing the image with boxes drawn.

r.save() ‚Üí Saves the image (like output.jpg) with detections.

üîπ Q8. What type of objects can YOLO detect?

A: The pre-trained YOLOv8 model can detect 80 common objects, including: person, dog, car, chair, phone, cat, etc.

üîπ Q9. What are the advantages of YOLO?

A: ‚úÖ Very fast (real-time capable) ‚úÖ Single-pass detection (detects all objects at once) ‚úÖ Works on images and videos ‚úÖ Pre-trained models available

üîπ Q10. What is the difference between YOLOv3 and YOLOv8?

A:

Feature YOLOv3 YOLOv8 Released 2018 2023 Framework Darknet PyTorch Accuracy Good Much better Speed Fast Faster Easy to use ‚ùå No ‚úÖ Yes (Ultralytics API)

YOLO (You Only Look Once) ‚Äì Summary & Architecture üîπ 1. Basic Concept

YOLO = You Only Look Once

A real-time object detection algorithm.

Detects multiple objects in one pass of the neural network.

Treats detection as a single regression problem (image ‚Üí bounding boxes + class labels).

Very fast and accurate compared to older methods (like R-CNN).

üîπ 2. Working Principle

The image is divided into an S √ó S grid (e.g., 13√ó13).

Each grid cell:

Predicts bounding boxes (x, y, width, height)

Predicts confidence score

Predicts class probabilities

All predictions are combined to get final object detections.

üîπ 3. YOLO Architecture (Simplified)

Input Layer

Takes image (e.g., 416√ó416√ó3).

Convolutional Layers

Extract features like edges, shapes, and colors.

Batch Normalization + Activation

Normalize and speed up learning (LeakyReLU used).

Pooling Layers

Reduce image size ‚Üí make computation faster.

Feature Extraction

Deep layers learn patterns and object details.

Detection Layer

Predicts bounding boxes and classes.

Output

Final list of detected objects with labels and confidence.

üîπ 4. Output of YOLO

Each detection gives:

[x_center, y_center, width, height, confidence, class probabilities]

üîπ 5. Key Features

Single forward pass ‚Üí very fast

End-to-end training

Detects multiple objects simultaneously

Real-time performance on GPU

üîπ 6. Components of YOLOv8

Backbone: CSPDarknet ‚Üí extracts features

Neck: PAN (Path Aggregation Network) ‚Üí combines multi-scale features

Head: Anchor-free detection ‚Üí predicts boxes & classes directly

üîπ 7. Advantages

‚úÖ Real-time detection ‚úÖ High accuracy ‚úÖ Works for images and videos ‚úÖ End-to-end trainable ‚úÖ Open-source and easy to use (Ultralytics API)

üîπ 8. Limitations

‚ö†Ô∏è Struggles with very small or overlapping objects ‚ö†Ô∏è Might misclassify objects with unclear boundaries

üîπ 9. Applications

Self-driving cars

CCTV / surveillance

Medical imaging

Agriculture (crop or animal detection)

Traffic monitoring

Mobile apps and robots

üîπ 10. YOLO Versions (Overview) Version Year Key Feature YOLOv1 2016 Original single-shot detector YOLOv2 2017 Improved accuracy & speed YOLOv3 2018 Multi-scale detection YOLOv4 2020 More accurate (CSPDarknet) YOLOv5 2021 PyTorch-based, user-friendly YOLOv7 2022 Speed‚Äìaccuracy optimized YOLOv8 2023 Anchor-free, best version yet üîπ 11. Key Technical Terms

Bounding Box: Rectangle around detected object

Confidence Score: How sure YOLO is about detection

Class Probability: Probability that the object belongs to a specific class

Non-Maximum Suppression (NMS): Removes duplicate boxes keeping only the best one

üîπ 12. Why YOLO is Popular

Combines speed + accuracy

Easy to implement

Works in real time

Supports multiple versions (v1‚Äìv8)

Pre-trained models available (no training needed for beginners)
